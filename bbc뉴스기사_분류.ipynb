{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bbc뉴스기사 분류.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMojy3hKpiWtz+gsNQ816nq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nohyunjin/DeepLearning/blob/main/bbc%EB%89%B4%EC%8A%A4%EA%B8%B0%EC%82%AC_%EB%B6%84%EB%A5%98.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "luHMP2g4V8s7"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "from time import time\n",
        "\n",
        "import nltk\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import LSTM, Embedding\n",
        "from keras.layers import Bidirectional"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MY_WORDS = 5000\n",
        "MY_EMBED = 64\n",
        "MY_HIDDEN = 100\n",
        "MY_LEN = 200\n",
        "\n",
        "MY_SPLIT = 0.8\n",
        "MY_SAMPLE = 123\n",
        "MY_EPOCH = 10"
      ],
      "metadata": {
        "id": "f9z3B-YAYFN5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original = []\n",
        "processed = []\n",
        "labels = []"
      ],
      "metadata": {
        "id": "aGqfQaCmYNSw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "MY_STOP = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "print(\"불용어 수:\", len(MY_STOP))\n",
        "print(MY_STOP)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJzw1nFIYRM0",
        "outputId": "9d9055ed-8178-41fc-c332-75df17084140"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "불용어 수: 179\n",
            "{'is', 'be', 'themselves', 'under', 'all', 'am', 'won', 'how', 'mightn', 'same', 'own', 'from', 'more', 'up', 'before', 'you', 'doing', 'that', 'because', 'above', \"it's\", 'over', 'against', 'yourself', 'don', 'shouldn', 'now', 'both', 'his', 'other', 'only', 'couldn', \"hadn't\", 'mustn', 'so', 'no', 'needn', 'will', 'during', \"she's\", \"wouldn't\", 'has', 'those', 'too', 's', 'there', 'had', 'why', 'an', 'o', 'can', 'weren', 'while', \"hasn't\", 'it', 'on', 'and', 'here', 'been', 'few', 'again', \"should've\", 'ma', 'for', 'to', 'we', \"isn't\", 'each', 'very', \"shan't\", 'll', 'himself', 're', 'do', 'after', \"you'll\", 'ours', \"doesn't\", 'didn', 'its', 'in', 'doesn', 'your', 'wouldn', 'our', 'not', 'aren', \"didn't\", 'being', 'than', 'my', 'did', 'any', 'their', 'them', 'once', 'if', 'through', 'i', 'haven', 'which', 'd', 'shan', 'but', 'me', 'by', 'of', 've', 'theirs', 'just', 'are', 'below', \"won't\", \"mightn't\", 'into', 'were', 'what', 'these', \"that'll\", 'between', 'whom', \"wasn't\", 'ain', 'should', 'or', 'itself', 'off', 'until', 'yourselves', \"you're\", \"mustn't\", 'nor', \"you've\", 't', 'having', 'further', 'then', 'he', \"weren't\", 'about', 'down', 'this', 'who', 'some', 'her', 'was', 'have', 'when', \"shouldn't\", 'where', \"aren't\", 'a', 'with', 'myself', 'ourselves', 'most', 'they', \"couldn't\", 'out', \"needn't\", 'y', 'hasn', 'wasn', 'him', 'at', 'such', 'yours', \"you'd\", 'the', 'she', 'herself', 'does', 'm', 'hers', 'as', 'hadn', 'isn', \"haven't\", \"don't\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('bbc-text.csv', 'r') as file :\n",
        "    reader = csv.reader(file)\n",
        "    next(reader)\n",
        "    for row in reader :\n",
        "        labels.append(row[0]) # 라벨\n",
        "        original.append(row[1]) # 원본 텍스트\n",
        "        news = row[1]\n",
        "        for word in MY_STOP:\n",
        "            token = ' ' + word + ' '\n",
        "            news = news.replace(token, ' ')\n",
        "        processed.append(news) # 불용어가 삭제된 텍스트"
      ],
      "metadata": {
        "id": "E-Lt8tToYcki"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = original[MY_SAMPLE]\n",
        "print('불용어 삭제 전 샘플 기사')\n",
        "print(sample)\n",
        "print('카테고리:', labels[MY_SAMPLE])\n",
        "print('단어 수:', len(sample.split()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SISC6SFKY3CZ",
        "outputId": "ca5dc0ac-811e-46f3-ac3a-3a7a244b38be"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 삭제 전 샘플 기사\n",
            "screensaver tackles spam websites net users are getting the chance to fight back against spam websites  internet portal lycos has made a screensaver that endlessly requests data from sites that sell the goods and services mentioned in spam e-mail. lycos hopes it will make the monthly bandwidth bills of spammers soar by keeping their servers running flat out. the net firm estimates that if enough people sign up and download the tool  spammers could end up paying to send out terabytes of data.   we ve never really solved the big problem of spam which is that its so damn cheap and easy to do   said malte pollmann  spokesman for lycos europe.  in the past we have built up the spam filtering systems for our users   he said   but now we are going to go one step further.    we ve found a way to make it much higher cost for spammers by putting a load on their servers.  by getting thousands of people to download and use the screensaver  lycos hopes to get spamming websites constantly running at almost full capacity. mr pollmann said there was no intention to stop the spam websites working by subjecting them with too much data to cope with. he said the screensaver had been carefully written to ensure that the amount of traffic it generated from each user did not overload the web.  every single user will contribute three to four megabytes per day   he said   about one mp3 file.  but  he said  if enough people sign up spamming websites could be force to pay for gigabytes of traffic every single day. lycos did not want to use e-mail to fight back  said mr pollmann.  that would be fighting one bad thing with another bad thing   he said.  the sites being targeted are those mentioned in spam e-mail messages and which sell the goods and services on offer.  typically these sites are different to those that used to send out spam e-mail and they typically only get a few thousand visitors per day. the list of sites that the screensaver will target is taken from real-time blacklists generated by organisations such as spamcop. to limit the chance of mistakes being made  lycos is using people to ensure that the sites are selling spam goods. as these sites rarely use advertising to offset hosting costs  the burden of high-bandwidth bills could make spam too expensive  said mr pollmann. sites will also slow down under the weight of data requests. early results show that response times of some sites have deteriorated by up to 85%. users do not have to be registered users of lycos to download and use the screensaver. while working  the screensaver shows the websites that are being bothered with requests for data. the screensaver is due to be launched across europe on 1 december and before now has only been trialled in sweden. despite the soft launch  mr pollmann said that the screensaver had been downloaded more than 20 000 times in the last four days.  there s a huge user demand to not only filter spam day-by-day but to do something more   he said  before now users have never had the chance to be a bit more offensive.\n",
            "카테고리: tech\n",
            "단어 수: 536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('불용어 삭제 후 샘플 기사')\n",
        "sample = processed[MY_SAMPLE]\n",
        "print(sample)\n",
        "print('단어 수:', len(sample.split()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7yBhhNtZFl3",
        "outputId": "c49df8f3-05e2-467a-e0ca-fcb773d7aff0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 삭제 후 샘플 기사\n",
            "screensaver tackles spam websites net users getting chance fight back spam websites  internet portal lycos made screensaver endlessly requests data sites sell goods services mentioned spam e-mail. lycos hopes make monthly bandwidth bills spammers soar keeping servers running flat out. net firm estimates enough people sign download tool  spammers could end paying send terabytes data.   never really solved big problem spam damn cheap easy   said malte pollmann  spokesman lycos europe.  past built spam filtering systems users   said   going go one step further.    found way make much higher cost spammers putting load servers.  getting thousands people download use screensaver  lycos hopes get spamming websites constantly running almost full capacity. mr pollmann said intention stop spam websites working subjecting much data cope with. said screensaver carefully written ensure amount traffic generated user overload web.  every single user contribute three four megabytes per day   said   one mp3 file.   said  enough people sign spamming websites could force pay gigabytes traffic every single day. lycos want use e-mail fight back  said mr pollmann.  would fighting one bad thing another bad thing   said.  sites targeted mentioned spam e-mail messages sell goods services offer.  typically sites different used send spam e-mail typically get thousand visitors per day. list sites screensaver target taken real-time blacklists generated organisations spamcop. limit chance mistakes made  lycos using people ensure sites selling spam goods. sites rarely use advertising offset hosting costs  burden high-bandwidth bills could make spam expensive  said mr pollmann. sites also slow weight data requests. early results show response times sites deteriorated 85%. users registered users lycos download use screensaver. working  screensaver shows websites bothered requests data. screensaver due launched across europe 1 december trialled sweden. despite soft launch  mr pollmann said screensaver downloaded 20 000 times last four days.  huge user demand filter spam day-by-day something   said  users never chance bit offensive.\n",
            "단어 수: 303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A_token = Tokenizer(num_words = MY_WORDS, oov_token = 'OOV')\n",
        "\n",
        "A_token.fit_on_texts(processed)\n",
        "A_tokenized = A_token.texts_to_sequences(processed)\n",
        "\n",
        "maxlen = max([len(x) for x in A_tokenized])\n",
        "minlen = min([len(x) for x in A_tokenized])\n",
        "\n",
        "print('토큰화 결과')\n",
        "print(f'총 단어 수: {len(A_token.word_counts)}')\n",
        "print(f'가장 긴 기사: {maxlen}')\n",
        "print(f'가장 짧은 기사: {minlen}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99pzOTVxZfK8",
        "outputId": "3fa5e856-3bb2-427c-ea55-c3f0476556de"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "토큰화 결과\n",
            "총 단어 수: 29698\n",
            "가장 긴 기사: 2279\n",
            "가장 짧은 기사: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A_tokenized = pad_sequences(A_tokenized, maxlen = MY_LEN, padding = 'post', truncating = 'post')\n",
        "A_tokenized = np.array(A_tokenized)\n",
        "\n",
        "sample = A_tokenized[MY_SAMPLE]\n",
        "print('샘플 기사 토큰화 후 패딩 결과')\n",
        "print(sample)\n",
        "print('단어 수:', len(sample))\n",
        "print('샘플 shape:', sample.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDIRH9DKaIu7",
        "outputId": "a5f3098d-2659-4484-9d38-7715748034d2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "샘플 기사 토큰화 후 패딩 결과\n",
            "[3171    1  816  877  115  136  382  347  716   28  816  877  228    1\n",
            " 3172   27 3171    1 4869  203  568  733 1770  126 4025  816  260  395\n",
            " 3172  700   21 1648 3629 2849 2607    1 2325 2551  453 2919  569  115\n",
            "   63 2291  381    7 1161  780 1859 2607   11   92 1570 1051    1  203\n",
            "  281  154    1  138  364  816    1 2224  847    2    1    1  178 3172\n",
            "  139  255 1110  816    1  726  136    2   52   60   10  818 3790  195\n",
            "   41   21   56  494  245 2607 1363    1 2551  382 1021    7  780   70\n",
            " 3171 3172  700   23    1  877 3993  453  343  322 1394    3    1    2\n",
            " 3428  582  816  877  297    1   56  203 2296 2357    2 3171 2709 1069\n",
            "  660  812 1287 3885 1538    1  466  224  503 1538    1   31   96    1\n",
            "  681  111    2   10 1897  912    2  381    7 1161    1  877   11  722\n",
            "  256    1 1287  224  503  111 3172   79   70  260  395  716   28    2\n",
            "    3    1    4 1604   10  823  455  158  823  455    2  568 2178 4025\n",
            "  816  260  395  891  733 1770  126  220 3677  568  316   86 1051  816\n",
            "  260  395 3677   23]\n",
            "단어 수: 200\n",
            "샘플 shape: (200,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "C_token = Tokenizer()\n",
        "C_token.fit_on_texts(labels)\n",
        "C_tokenized = C_token.texts_to_sequences(labels)\n",
        "C_tokenized = np.array(C_tokenized).reshape(-1)\n",
        "\n",
        "print('카테고리 토큰화 결과')\n",
        "print(C_token.word_index)\n",
        "print('샘플 기사 카테고리:', C_tokenized[MY_SAMPLE])\n",
        "print('토큰화 결과 shape:', C_tokenized.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LF99xT4-ajsY",
        "outputId": "b6b808e7-4867-4c21-8add-c42353146f5d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "카테고리 토큰화 결과\n",
            "{'sport': 1, 'business': 2, 'politics': 3, 'tech': 4, 'entertainment': 5}\n",
            "샘플 기사 카테고리: 4\n",
            "토큰화 결과 shape: (2225,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx_to_label = {}\n",
        "for label, index in C_token.word_index.items():\n",
        "    idx_to_label[index] = label"
      ],
      "metadata": {
        "id": "61qWT79ocHbd"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(A_tokenized, C_tokenized, train_size = MY_SPLIT, shuffle = False)\n",
        "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gykk6n0idGqw",
        "outputId": "e265369c-3a98-4a15-df87-4636e867dfbd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1780, 200) (1780,) (445, 200) (445,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim = MY_WORDS, output_dim = MY_EMBED))\n",
        "model.add(Dropout(rate = 0.5))\n",
        "model.add(Bidirectional(LSTM(units = MY_HIDDEN)))\n",
        "model.add(Dense(units = 6, activation = 'softmax'))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wzfv3si5dazw",
        "outputId": "672ee37b-d1bc-40a0-e46e-1e44fc859f9e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 64)          320000    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, None, 64)          0         \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 200)              132000    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 6)                 1206      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 453,206\n",
            "Trainable params: 453,206\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['acc'])"
      ],
      "metadata": {
        "id": "Al79lXRNhM5H"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "begin = time()\n",
        "model.fit(x=X_train, y = Y_train, epochs = MY_EPOCH, verbose = 2)\n",
        "end = time()\n",
        "print(f'학습시간: {end-begin:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTaq7S13hegM",
        "outputId": "cf0f7dde-ef96-42a1-e4d4-1dae44cfc551"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "56/56 - 6s - loss: 1.5939 - acc: 0.2708 - 6s/epoch - 116ms/step\n",
            "Epoch 2/10\n",
            "56/56 - 1s - loss: 1.2381 - acc: 0.4904 - 1s/epoch - 18ms/step\n",
            "Epoch 3/10\n",
            "56/56 - 1s - loss: 0.5584 - acc: 0.8067 - 1s/epoch - 19ms/step\n",
            "Epoch 4/10\n",
            "56/56 - 1s - loss: 0.5187 - acc: 0.8427 - 1s/epoch - 19ms/step\n",
            "Epoch 5/10\n",
            "56/56 - 1s - loss: 0.1979 - acc: 0.9562 - 1s/epoch - 19ms/step\n",
            "Epoch 6/10\n",
            "56/56 - 1s - loss: 0.1010 - acc: 0.9764 - 1s/epoch - 19ms/step\n",
            "Epoch 7/10\n",
            "56/56 - 1s - loss: 0.0444 - acc: 0.9933 - 1s/epoch - 19ms/step\n",
            "Epoch 8/10\n",
            "56/56 - 1s - loss: 0.0467 - acc: 0.9904 - 1s/epoch - 19ms/step\n",
            "Epoch 9/10\n",
            "56/56 - 1s - loss: 0.0140 - acc: 0.9989 - 1s/epoch - 19ms/step\n",
            "Epoch 10/10\n",
            "56/56 - 1s - loss: 0.0062 - acc: 1.0000 - 1s/epoch - 19ms/step\n",
            "학습시간: 16.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(x = X_test, y = Y_test, verbose = 2)\n",
        "print(f'최종 정확도: {score[1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-C68iVyhxc1",
        "outputId": "93608167-b204-41ad-a38b-b132a8b83a21"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14/14 - 1s - loss: 0.1539 - acc: 0.9618 - 800ms/epoch - 57ms/step\n",
            "최종 정확도: 0.9617977738380432\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.predict(x = X_test)\n",
        "pred = np.argmax(pred, axis = 1)\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5en_p6Wh5ra",
        "outputId": "b065a6db-450a-48cc-e8b4-8287f25cf2a5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5 4 3 1 1 4 2 4 5 5 3 3 2 5 1 5 5 2 1 3 4 2 1 5 4 3 3 1 1 3 2 2 2 2 5 2 3\n",
            " 3 4 4 5 3 5 2 3 1 1 2 4 2 4 1 2 2 3 1 1 3 2 5 5 3 2 2 3 2 4 2 3 3 3 3 5 5\n",
            " 4 3 1 1 1 4 5 1 1 5 4 5 4 1 4 1 1 5 5 2 5 5 3 2 1 4 4 3 2 1 2 5 1 3 5 1 1\n",
            " 2 3 4 4 2 2 1 3 5 1 1 3 5 4 1 5 2 3 1 3 4 5 1 3 2 5 3 5 3 1 3 2 2 3 2 4 1\n",
            " 2 5 2 1 1 5 4 3 4 3 3 1 1 1 2 4 5 2 1 2 1 2 4 2 2 2 2 1 1 1 2 2 5 2 2 2 1\n",
            " 1 1 4 2 1 1 1 2 5 4 4 4 3 2 2 4 2 4 1 1 3 3 3 1 1 3 3 4 2 1 1 1 1 2 1 2 2\n",
            " 2 2 1 3 1 4 4 1 4 2 5 2 1 2 4 4 3 5 2 5 2 4 3 5 2 5 5 4 2 4 4 2 3 1 5 2 3\n",
            " 5 2 4 1 4 3 1 3 2 3 3 2 2 2 4 3 2 3 2 5 3 1 3 3 1 5 4 4 2 4 1 2 2 2 1 4 4\n",
            " 4 1 5 1 3 2 3 3 5 4 2 4 1 5 5 1 2 5 4 4 1 5 2 3 3 3 4 4 2 3 2 4 3 5 5 2 2\n",
            " 4 5 4 4 1 3 1 1 3 5 5 2 3 3 1 2 2 4 2 4 4 1 2 3 1 2 2 1 4 1 4 5 1 1 5 2 4\n",
            " 1 1 3 4 2 3 1 1 3 2 4 4 2 2 1 5 5 4 2 3 4 1 1 4 4 3 2 1 5 5 1 3 4 1 2 2 2\n",
            " 1 1 4 1 2 4 2 2 1 2 3 2 2 5 3 4 3 4 5 3 4 5 1 3 5 2 4 2 4 5 4 1 2 2 3 5 3\n",
            " 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_true = Y_test, y_pred = pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lB_Eh_Zrh-6e",
        "outputId": "fa6ca9b7-888b-4ff5-982f-2ef7b0461b7c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 99   0   0   0   2]\n",
            " [  0 104   2   0   0]\n",
            " [  1   5  79   1   0]\n",
            " [  1   2   0  82   1]\n",
            " [  0   1   1   0  64]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "news = ['''\n",
        "The International Space Station (ISS) will continue working until 2030, before plunging into the Pacific Ocean in early 2031, according to Nasa.\n",
        "\n",
        "In a report this week, the US space agency said the ISS would crash into a part of the ocean known as Point Nemo.\n",
        "\n",
        "This is the point furthest from land on planet Earth, also known as the spacecraft cemetery.\n",
        "\n",
        "Many old satellites and other space debris have crashed there, including the Russian space station Mir in 2001.\n",
        "\n",
        "Nasa said that in the future space activities close to Earth would be led by the commercial sector.\n",
        "\n",
        "The ISS - a joint project involving five space agencies - has been in orbit since 1998 and has been continuously crewed since 2000. More than 3,000 research investigations have taken place in its microgravity laboratory.\n",
        "'''.lower()]\n",
        "\n",
        "news[0] = re.sub(r'[^\\w\\s]','', news[0])\n",
        "\n",
        "print(\"불용어 처리 전 문자 수:\", len(news[0]))\n",
        "for word in MY_STOP:\n",
        "    token = ' ' + word + ' '\n",
        "    news[0] = news[0].replace(token, ' ')\n",
        "print(\"불용어 처리 후 문자 수:\", len(news[0]))\n",
        "\n",
        "seq = A_token.texts_to_sequences(news)\n",
        "padded = pad_sequences(seq, maxlen = MY_LEN, padding = 'post', truncating = 'post')\n",
        "\n",
        "my_pred = model.predict(padded)\n",
        "print(\"예측 결과:\", idx_to_label[np.argmax(my_pred[0])])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wludeZpeiEnX",
        "outputId": "c8a3069b-0392-4c68-d03c-292f71e61fec"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 처리 전 문자 수: 774\n",
            "불용어 처리 후 문자 수: 585\n",
            "예측 결과: tech\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uW0yvY1EkJYB"
      }
    }
  ]
}